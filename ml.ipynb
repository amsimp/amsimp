{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xarray/core/merge.py:10: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)\n"
     ]
    }
   ],
   "source": [
    "# Import dependices.\n",
    "import iris\n",
    "from iris.coords import AuxCoord\n",
    "from iris.cube import CubeList\n",
    "import xarray as xr\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import ConvLSTM2D, Dense, TimeDistributed\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure reproducibility.\n",
    "tf.random.set_seed(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split data into windows.\n",
    "def window_data(dataset, past_history, future_target):\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(dataset.shape[0]):\n",
    "        # Find the end.\n",
    "        end_ix = i + past_history\n",
    "        out_end_ix = end_ix + future_target\n",
    "        \n",
    "        # Determine if we are beyond the dataset.\n",
    "        if out_end_ix > dataset.shape[0]:\n",
    "            break\n",
    "        \n",
    "        # Gather the input and output components.\n",
    "        seq_x, seq_y = dataset[i:end_ix], dataset[end_ix:out_end_ix]\n",
    "\n",
    "        # Append to list.\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, ds, past_history, batch_size=32, shuffle=True, load=True, mean=None, std=None):\n",
    "        \"\"\"\n",
    "        Template from https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "        \n",
    "        Args:\n",
    "            ds: Dataset containing all variables\n",
    "            past_history: how much of the past in hours is shown to the model\n",
    "            batch_size: Batch size\n",
    "            shuffle: bool. If True, data is shuffled.\n",
    "            load: bool. If True, datadet is loaded into RAM.\n",
    "            mean: If None, compute mean from data.\n",
    "            std: If None, compute standard deviation from data.\n",
    "        \"\"\"\n",
    "        self.ds = ds\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        data = []\n",
    "        generic_level = xr.DataArray([1], coords={'level': [1]}, dims=['level'])\n",
    "        data.append(ds.expand_dims({'level': generic_level}, 1))\n",
    "\n",
    "        self.data = xr.concat(data, 'level').transpose('time', 'latitude', 'longitude', 'level')\n",
    "        self.mean = self.data.mean(('time', 'latitude', 'longitude')).compute() if mean is None else mean\n",
    "        self.std = self.data.std('time').mean(('latitude', 'longitude')).compute() if std is None else std\n",
    "        # Normalize\n",
    "        self.data = (self.data - self.mean) / self.std\n",
    "        \n",
    "        # Window data.\n",
    "        self.window = window_data(self.data, past_history, 1)\n",
    "\n",
    "        self.n_samples = len(self.window[0])\n",
    "        self.on_epoch_end()\n",
    "\n",
    "        # For some weird reason calling .load() earlier messes up the mean and std computations\n",
    "        if load: print('Loading data into RAM'); self.data.load()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.ceil(self.n_samples / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        'Generate one batch of data'\n",
    "        idxs = self.idxs[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "\n",
    "        X, y = self.window[0], self.window[1]\n",
    "        X = np.array([X[k].values for k in idxs])\n",
    "        y = np.array([y[k].values for k in idxs])\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.idxs = np.arange(self.n_samples)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define root mean squared error loss function\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data.\n",
    "folder = 'dataset/'\n",
    "\n",
    "# Time steps.\n",
    "nsteps = 600\n",
    "\n",
    "# Parameter label.\n",
    "parameter_label = \"2m_temperature\"\n",
    "\n",
    "# Load dataset.\n",
    "dataset = iris.load(folder + \"/\" + parameter_label + '/*.nc')[0][:nsteps]\n",
    "\n",
    "# Convert cube to data array.\n",
    "dataset = xr.DataArray.from_iris(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data, past_history, epochs, bs):\n",
    "    # Define number of latitude and longitude points.\n",
    "    nlat, nlon = data.shape[1], data.shape[2]\n",
    "    \n",
    "    # Define training dataset and validation dataset.\n",
    "    train_dataset = data[:int(0.7 * data.shape[0])]\n",
    "    val_dataset = data[int(0.7 * data.shape[0]):int(0.9 * data.shape[0])]\n",
    "    \n",
    "    # Training dataset.\n",
    "    train_generator = DataGenerator(\n",
    "        train_dataset, \n",
    "        past_history, \n",
    "        batch_size=bs, \n",
    "        load=False\n",
    "    )\n",
    "    \n",
    "    # Validation dataset.\n",
    "    val_generator = DataGenerator(\n",
    "        val_dataset, \n",
    "        past_history, \n",
    "        batch_size=bs, \n",
    "        mean=train_generator.mean, \n",
    "        std=train_generator.std,\n",
    "        load=False,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # Create, and train models.\n",
    "    # Optimiser.\n",
    "    opt = Adam(lr=1e-3, decay=1e-5)\n",
    "    # Create model.\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First layer.\n",
    "    model.add(\n",
    "        ConvLSTM2D(\n",
    "            filters=64, \n",
    "            kernel_size=(7, 7),\n",
    "            input_shape=(past_history, nlat, nlon, 1), \n",
    "            padding='same', \n",
    "            return_sequences=True, \n",
    "            activation='tanh', \n",
    "            recurrent_activation='hard_sigmoid',\n",
    "            kernel_initializer='glorot_uniform', \n",
    "            unit_forget_bias=True, \n",
    "            dropout=0.3, \n",
    "            recurrent_dropout=0.3, \n",
    "            go_backwards=True\n",
    "        )\n",
    "    )\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Second layer.\n",
    "    model.add(\n",
    "        ConvLSTM2D(\n",
    "            filters=32, \n",
    "            kernel_size=(7, 7), \n",
    "            padding='same', \n",
    "            return_sequences=True, \n",
    "            activation='tanh', \n",
    "            recurrent_activation='hard_sigmoid', \n",
    "            kernel_initializer='glorot_uniform', \n",
    "            unit_forget_bias=True, \n",
    "            dropout=0.4, \n",
    "            recurrent_dropout=0.3, \n",
    "            go_backwards=True\n",
    "        )\n",
    "    )\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Third layer.\n",
    "    model.add(\n",
    "        ConvLSTM2D(\n",
    "            filters=32, \n",
    "            kernel_size=(7, 7), \n",
    "            padding='same', \n",
    "            return_sequences=True, \n",
    "            activation='tanh', \n",
    "            recurrent_activation='hard_sigmoid', \n",
    "            kernel_initializer='glorot_uniform', \n",
    "            unit_forget_bias=True, \n",
    "            dropout=0.4, \n",
    "            recurrent_dropout=0.3, \n",
    "            go_backwards=True\n",
    "        )\n",
    "    )\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Final layer.\n",
    "    model.add(\n",
    "        ConvLSTM2D(\n",
    "            filters=32, \n",
    "            kernel_size=(7, 7), \n",
    "            padding='same', \n",
    "            return_sequences=False, \n",
    "            activation='tanh', \n",
    "            recurrent_activation='hard_sigmoid', \n",
    "            kernel_initializer='glorot_uniform', \n",
    "            unit_forget_bias=True, \n",
    "            dropout=0.4, \n",
    "            recurrent_dropout=0.3, \n",
    "            go_backwards=True\n",
    "        )\n",
    "    )\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Add dense layer.\n",
    "    model.add(Dense(1)) \n",
    "    \n",
    "    # Compile model.\n",
    "    model.compile(\n",
    "        optimizer=opt, loss=rmse, metrics=['mean_absolute_error', 'mse']\n",
    "    )\n",
    "    \n",
    "    # Summary of model.\n",
    "    model.summary()\n",
    "\n",
    "    # Train.\n",
    "    model.fit(\n",
    "        train_generator, \n",
    "        validation_data=val_generator,\n",
    "        epochs=epochs,\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                min_delta=0,\n",
    "                patience=2, \n",
    "                mode='auto'\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_lst_m2d (ConvLSTM2D)    (None, 84, 37, 72, 64)    815616    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 84, 37, 72, 64)    256       \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_1 (ConvLSTM2D)  (None, 84, 37, 72, 32)    602240    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 84, 37, 72, 32)    128       \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_2 (ConvLSTM2D)  (None, 84, 37, 72, 32)    401536    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 84, 37, 72, 32)    128       \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_3 (ConvLSTM2D)  (None, 37, 72, 32)        401536    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 37, 72, 32)        128       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 37, 72, 1)         33        \n",
      "=================================================================\n",
      "Total params: 2,221,601\n",
      "Trainable params: 2,221,281\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n"
     ]
    }
   ],
   "source": [
    "# Define model.\n",
    "model = model(dataset, (15*12), epochs=100, bs=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m56"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
