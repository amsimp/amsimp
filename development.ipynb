{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependices.\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import ConvLSTM2D, Dropout, Dense\n",
    "from tensorflow.keras.layers import BatchNormalization, MaxPooling3D, UpSampling3D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure reproducibility.\n",
    "tf.random.set_seed(13)\n",
    "\n",
    "# Enable multi-GPU support.\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# Enable accelerated linear algebra.\n",
    "tf.config.optimizer.set_jit(True)\n",
    "\n",
    "# Enable mixed precision.\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, lst_n, batch_size=32, shuffle=True):\n",
    "        \"\"\"\n",
    "        Template from https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "        \"\"\"\n",
    "        # Define variables.\n",
    "        self.lst_n = lst_n\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.folder = \"processed_dataset/\"\n",
    "        self.fname = \"sample\"\n",
    "        self.shape = (self.batch_size, 6, 721, 1440, 4)\n",
    "        \n",
    "        # Extra variables.\n",
    "        self.n_samples = lst_n.shape[0]\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.ceil(self.n_samples / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        'Generate one batch of data'\n",
    "        idxs = self.idxs[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "        \n",
    "        # Define outputs.\n",
    "        X = np.zeros(self.shape)\n",
    "        y = np.zeros(self.shape)\n",
    "\n",
    "        # Get file names and load.\n",
    "        n = 0\n",
    "        for k in idxs:\n",
    "            # Define file name.\n",
    "            fname = self.fname + str(int(self.lst_n[k])) + \".npy\"\n",
    "        \n",
    "            # Load file.\n",
    "            file = np.load(self.folder + fname)\n",
    "            \n",
    "            # Reduce spatial resolution.\n",
    "            X[n] = file[0]\n",
    "            y[n] = file[1]\n",
    "            \n",
    "            # Increment.\n",
    "            n += 1\n",
    "        \n",
    "        # Remove total precipitation.\n",
    "        X = np.delete(X, 1, axis=4)\n",
    "        y = np.delete(y, 1, axis=4)\n",
    "        \n",
    "        return X[:, :, 4:-3:4, ::4, :], y[:, :, 4:-3:4, ::4, :]\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.idxs = np.arange(self.n_samples)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(epochs, bs):\n",
    "    # Number of elements.\n",
    "    n = len(os.listdir(\"processed_dataset/\"))\n",
    "    lst_n = np.linspace(1, n, n)\n",
    "    \n",
    "    # Define training dataset and validation dataset.\n",
    "    # Training.\n",
    "    train_ns = lst_n[:int(0.7 * n)]\n",
    "    train_generator = DataGenerator(\n",
    "        train_ns,\n",
    "        batch_size=bs,\n",
    "    )\n",
    "    print(\"Training dataset created.\")\n",
    "    \n",
    "    # Validation.\n",
    "    val_ns = lst_n[int(0.7 * n):int(0.9 * n)]\n",
    "    val_generator = DataGenerator(\n",
    "        val_ns, \n",
    "        batch_size=bs,\n",
    "        shuffle=False\n",
    "    )\n",
    "    print(\"Validation dataset created.\")\n",
    "    \n",
    "    with mirrored_strategy.scope():\n",
    "        # Create, and train models.\n",
    "        # Optimiser.\n",
    "        opt = Adam(lr=1e-3, decay=1e-5)\n",
    "        # Create model.\n",
    "        model = Sequential()\n",
    "\n",
    "        # First layer.\n",
    "        model.add(\n",
    "            ConvLSTM2D(\n",
    "                filters=64, \n",
    "                kernel_size=(7, 7),\n",
    "                input_shape=(6, 179, 360, 3), \n",
    "                padding='same', \n",
    "                return_sequences=True, \n",
    "                activation='tanh', \n",
    "                recurrent_activation='hard_sigmoid',\n",
    "                kernel_initializer='glorot_uniform', \n",
    "                unit_forget_bias=True, \n",
    "                dropout=0.3, \n",
    "                recurrent_dropout=0.3, \n",
    "                go_backwards=True\n",
    "            )\n",
    "        )\n",
    "        # Batch normalisation.\n",
    "        model.add(BatchNormalization())\n",
    "        # Dropout.\n",
    "        model.add(Dropout(0.1))\n",
    "        \n",
    "        # Second layer.\n",
    "        model.add(\n",
    "            ConvLSTM2D(\n",
    "                filters=32, \n",
    "                kernel_size=(7, 7), \n",
    "                padding='same', \n",
    "                return_sequences=True, \n",
    "                activation='tanh', \n",
    "                recurrent_activation='hard_sigmoid', \n",
    "                kernel_initializer='glorot_uniform', \n",
    "                unit_forget_bias=True, \n",
    "                dropout=0.4, \n",
    "                recurrent_dropout=0.3, \n",
    "                go_backwards=True\n",
    "            )\n",
    "        )\n",
    "        # Batch normalisation.\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        # Third layer.\n",
    "        model.add(\n",
    "            ConvLSTM2D(\n",
    "                filters=32, \n",
    "                kernel_size=(7, 7), \n",
    "                padding='same', \n",
    "                return_sequences=True, \n",
    "                activation='tanh', \n",
    "                recurrent_activation='hard_sigmoid', \n",
    "                kernel_initializer='glorot_uniform', \n",
    "                unit_forget_bias=True, \n",
    "                dropout=0.4, \n",
    "                recurrent_dropout=0.3, \n",
    "                go_backwards=True\n",
    "            )\n",
    "        )\n",
    "        # Batch normalisation.\n",
    "        model.add(BatchNormalization())\n",
    "        # Dropout.\n",
    "        model.add(Dropout(0.1))\n",
    "\n",
    "        # Final layer.\n",
    "        model.add(\n",
    "            ConvLSTM2D(\n",
    "                filters=32, \n",
    "                kernel_size=(7, 7), \n",
    "                padding='same', \n",
    "                return_sequences=True, \n",
    "                activation='tanh', \n",
    "                recurrent_activation='hard_sigmoid', \n",
    "                kernel_initializer='glorot_uniform', \n",
    "                unit_forget_bias=True, \n",
    "                dropout=0.5, \n",
    "                recurrent_dropout=0.3, \n",
    "                go_backwards=True\n",
    "            )\n",
    "        )\n",
    "        # Batch normalisation.\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "        # Add dense layer.\n",
    "        model.add(Dense(3))\n",
    "    \n",
    "    # Compile model.\n",
    "    model.compile(\n",
    "        optimizer=opt, \n",
    "        loss='mse'\n",
    "    )\n",
    "    # Summary of model.\n",
    "    model.summary()\n",
    "    \n",
    "    # Load previous model weights.\n",
    "    model.load_weights('model/global_forecast_model.h5')\n",
    "\n",
    "    # Train.\n",
    "    model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=train_ns.shape[0] // epochs,\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=val_ns.shape[0] // epochs,\n",
    "        epochs=epochs\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model.\n",
    "model = model(epochs=30, bs=4)\n",
    "\n",
    "# Save model.\n",
    "model.save_weights('model/global_forecast_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m56"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
