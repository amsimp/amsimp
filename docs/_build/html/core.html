
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
  
  <!-- Licensed under the Apache 2.0 License -->
  <link rel="stylesheet" type="text/css" href="_static/fonts/open-sans/stylesheet.css" />
  <!-- Licensed under the SIL Open Font License -->
  <link rel="stylesheet" type="text/css" href="_static/fonts/source-serif-pro/source-serif-pro.css" />
  <link rel="stylesheet" type="text/css" href="_static/css/bootstrap.min.css" />
  <link rel="stylesheet" type="text/css" href="_static/css/bootstrap-theme.min.css" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
    <title>Dynamical Core &#8212; AMSIMP 0.4.2 documentation</title>
    <link rel="stylesheet" href="_static/guzzle.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Mathematics of AMSIMP" href="math.html" />
    <link rel="prev" title="Visualisations" href="visualisations.html" />
  
   

  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="math.html" title="Mathematics of AMSIMP"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="visualisations.html" title="Visualisations"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">AMSIMP 0.4.2 documentation</a> &#187;</li> 
      </ul>
    </div>
    <div class="container-wrapper">

      <div id="mobile-toggle">
        <a href="#"><span class="glyphicon glyphicon-align-justify" aria-hidden="true"></span></a>
      </div>
  <div id="left-column">
    <div class="sphinxsidebar">
<div class="sidebar-block">
  <div class="sidebar-wrapper">
    <h2>Contents</h2>
    <div class="sidebar-localtoc">
      <ul>
<li><a class="reference internal" href="#">Dynamical Core</a><ul>
<li><a class="reference internal" href="#finite-difference-method">Finite Difference Method</a><ul>
<li><a class="reference internal" href="#introduction">Introduction</a></li>
<li><a class="reference internal" href="#derivation-of-the-finite-difference-method">Derivation of the Finite Difference Method</a></li>
<li><a class="reference internal" href="#ftcs-scheme">FTCS Scheme</a></li>
<li><a class="reference internal" href="#leapfrog-scheme">Leapfrog Scheme</a></li>
<li><a class="reference internal" href="#nonlinear-instability">Nonlinear Instability</a></li>
</ul>
</li>
<li><a class="reference internal" href="#ensemble-prediction-system">Ensemble Prediction System</a><ul>
<li><a class="reference internal" href="#introduction-1">Introduction</a></li>
<li><a class="reference internal" href="#advantages-of-eps">Advantages of EPS</a></li>
<li><a class="reference internal" href="#global-eps">Global EPS</a></li>
</ul>
</li>
<li><a class="reference internal" href="#recurrent-neural-network">Recurrent Neural Network</a><ul>
<li><a class="reference internal" href="#introduction-2">Introduction</a></li>
<li><a class="reference internal" href="#lstm">LSTM</a></li>
<li><a class="reference internal" href="#implementation">Implementation</a></li>
</ul>
</li>
<li><a class="reference internal" href="#initial-conditions">Initial Conditions</a><ul>
<li><a class="reference internal" href="#global-data-assimilation-system">Global Data Assimilation System</a></li>
<li><a class="reference internal" href="#isobaric-co-ordinates">Isobaric Co-Ordinates</a></li>
</ul>
</li>
</ul>
</li>
</ul>

    </div>
  </div>
</div>
  <h4>Previous topic</h4>
  <p class="topless"><a href="visualisations.html"
                        title="previous chapter">Visualisations</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="math.html"
                        title="next chapter">Mathematics of AMSIMP</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/core.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div class="sidebar-block">
  <div class="sidebar-wrapper">
    <div id="main-search">
      <form class="form-inline" action="search.html" method="GET" role="form">
        <div class="input-group">
          <input name="q" type="text" class="form-control" placeholder="Search...">
        </div>
        <input type="hidden" name="check_keywords" value="yes" />
        <input type="hidden" name="area" value="default" />
      </form>
    </div>
  </div>
</div>
      
    </div>
  </div>
        <div id="right-column">
          
          <div role="navigation" aria-label="breadcrumbs navigation">
            <ol class="breadcrumb">
              <li><a href="index.html">Docs</a></li>
              
              <li>Dynamical Core</li>
            </ol>
          </div>
          
          <div class="document clearer body">
            
  <div class="section" id="dynamical-core">
<span id="fdm-section"></span><h1>Dynamical Core<a class="headerlink" href="#dynamical-core" title="Permalink to this headline">¶</a></h1>
<div class="section" id="finite-difference-method">
<h2>Finite Difference Method<a class="headerlink" href="#finite-difference-method" title="Permalink to this headline">¶</a></h2>
<div class="section" id="introduction">
<h3>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h3>
<p>The use of numerical models for the simulation of dynamics within the
atmosphere typically involves the solution of a set of partial
differential equations. These equations generally describe three
processes: advection, adjustment and diffusion.</p>
<p>Advection is the transport of a substance by bulk motion.</p>
<p>Adjustment is how the mass and wind fields adjust to one another.</p>
<p>Diffusion is the movement of a liquid, or gas from an area of high
concentration to an area of low concentration.</p>
<p>Most meteorological problems involving partial differential equations
generally fall into three distinct categories: initial value problems,
boundary value problems, and eigenvalue problems. The meteorological
problems associated with this project are solely initial value
problems <span class="bibtex" id="id1">[numerical_methods]</span>. An initial value problem
is a situation where you want to predict the future state of a given
system, given the necessary initial conditions. Unfortunately, the
equations describing the evolution of the atmosphere do not have exact
analytical solutions.</p>
<p>An analytical function is the most precise way of representing a
physical field as it gives us the value of this field at any point in
space, and at any instant in time.</p>
<p>When an analytical solution does not exist, an approximate numerical
solution is found using a specified computational
technique <span class="bibtex" id="id2">[numerical_methods]</span>. For the purposes of this
project, the finite difference method will be utilised. It must be
noted, however, that there are several other methods to acquire an
approximate numerical solution, with the finite element method and the
spectral method being just a few.</p>
<p>The idea of the finite difference method is to approximate the
derivatives in the partial differential equations with differences
between adjacent points in space and in time. The advantages of this
being that the problem becomes an algebraic one, and that a continuous
problem becomes a discrete one.</p>
</div>
<div class="section" id="derivation-of-the-finite-difference-method">
<h3>Derivation of the Finite Difference Method<a class="headerlink" href="#derivation-of-the-finite-difference-method" title="Permalink to this headline">¶</a></h3>
<p>In order to derive the finite difference method, it is necessary to look
at the Taylor series expansion of the function.</p>
<p>The Taylor series of a function is the limit of that function’s Taylor
polynomials as the degree increases.</p>
<p>The Taylor series expansion of <span class="math notranslate nohighlight">\(f(x)\)</span> can be represented as the
following:</p>
<div class="math notranslate nohighlight">
\[f(x + \Delta x) = f(x) + \frac{d f}{d x}(x) \frac{\Delta x}{1!} + \frac{d^2 f}{d x^2}(x) \frac{\Delta x^2}{2!} + ... + \frac{d^n f}{d x^n}(x) \frac{\Delta x^n}{n!} + ...\]</div>
<div class="math notranslate nohighlight">
\[f(x - \Delta x) = f(x) + \frac{d f}{d x}(x) \frac{-\Delta x}{1!} + \frac{d^2 f}{d x^2}(x) \frac{(-\Delta x)^2}{2!} + ... + \frac{d^n f}{d x^n}(x) \frac{(-\Delta x)^n}{n!} + ...\]</div>
<p>The higher order terms, which will be represented as
<span class="math notranslate nohighlight">\(\mathcal{O}(\Delta x)\)</span> from this point on, become less important
as <span class="math notranslate nohighlight">\(\Delta x\)</span> approaches zero.</p>
<div class="math notranslate nohighlight" id="equation-fds">
<span class="eqno">(1)<a class="headerlink" href="#equation-fds" title="Permalink to this equation">¶</a></span>\[\Rightarrow f(x + \Delta x) = f(x) + \frac{d f}{d x}(x) \frac{\Delta x}{1!} + \frac{d^2 f}{d x^2}(x) \frac{\Delta x^2}{2!} + [\mathcal{O}(\Delta x^3)]\]</div>
<div class="math notranslate nohighlight" id="equation-bds">
<span class="eqno">(2)<a class="headerlink" href="#equation-bds" title="Permalink to this equation">¶</a></span>\[\Rightarrow f(x - \Delta x) = f(x) - \frac{d f}{d x}(x) \frac{\Delta x}{1!} + \frac{d^2 f}{d x^2}(x) \frac{\Delta x^2}{2!} + [\mathcal{O}(\Delta x^3)]\]</div>
<p>These higher order terms are neglected, and the following approximation
for the derivative of <span class="math notranslate nohighlight">\(f(x)\)</span> is found:</p>
<div class="math notranslate nohighlight">
\[\Rightarrow \frac{d f}{d x}(x) = \frac{f(x + \Delta x) - f(x)}{\Delta x} + \mathcal{O}(\Delta x)\]</div>
<div class="math notranslate nohighlight">
\[\Rightarrow \frac{d f}{d x}(x) = \frac{f(x - \Delta x) - f(x)}{\Delta x} + \mathcal{O}(\Delta x)\]</div>
<p>These are called the forward and backward difference schemes
respectively, and by keeping only the leading order terms, an error of
order <span class="math notranslate nohighlight">\(\mathcal{O}(\Delta x)\)</span> is occurred. It is possible to
obtain a better approximation by subtracting <a class="reference internal" href="#equation-bds">(2)</a> from
<a class="reference internal" href="#equation-fds">(1)</a>, which yields equation <a class="reference internal" href="#equation-eq-fds-bds-subtract">(3)</a>.</p>
<div class="math notranslate nohighlight" id="equation-eq-fds-bds-subtract">
<span class="eqno">(3)<a class="headerlink" href="#equation-eq-fds-bds-subtract" title="Permalink to this equation">¶</a></span>\[\frac{d f}{d x}(x) = \frac{f(x + \Delta x) - f(x - \Delta x)}{\Delta x} + \mathcal{O}(\Delta x^2)\]</div>
<p>This particular approximation is called the central difference scheme,
and has an error of order <span class="math notranslate nohighlight">\(\mathcal{O}(\Delta x^2)\)</span>. Therefore,
this scheme is more accurate than the previously mentioned forward and
backward difference schemes. It is possible to take more and more terms
from the Taylor series expansion, however, there is an inherent trade
off between accuracy and computational efficiency.</p>
<p>In relation to the atmosphere, what this method does is divide the
atmosphere into several discrete horizontal layers, and each layer is
divided up into grid cells. Following which, each equation is evaluated
at the centre of the cell. Similarly, the time interval under
consideration is sliced into a number of discrete time steps. The size
of the grid step <span class="math notranslate nohighlight">\(\Delta x\)</span> and time step <span class="math notranslate nohighlight">\(\Delta t\)</span>
determines the accuracy of the scheme, with accuracy increasing as
<span class="math notranslate nohighlight">\(\Delta x\)</span> and <span class="math notranslate nohighlight">\(\Delta t\)</span> approach zero. On a synoptic
scale, <span class="math notranslate nohighlight">\(\Delta x\)</span> is generally equal to 500 km. For higher
resolutions, the grid-size is smaller, which corresponds to a greater
computational burden. As such, there is a trade off between accuracy and
computational performance. For Eulerian schemes, the typical time step
is 2 minutes. As such, since the software will use an Eulerian scheme,
the time step will be 2 minutes <a class="bibtex reference internal" href="refs.html#leapfrog-slides-one" id="id3">[12]</a>.</p>
</div>
<div class="section" id="ftcs-scheme">
<span id="ftcs-section"></span><h3>FTCS Scheme<a class="headerlink" href="#ftcs-scheme" title="Permalink to this headline">¶</a></h3>
<p>Given the information mentioned in the previous section, the most
obvious scheme to approximate a differential equation, which will be
used to predict the future state of the atmosphere, would be to combine
the central difference scheme for space and the forward difference
scheme for time (FTCS). This scheme would allow us access to the
increased accuracy of the central difference scheme, while maintaining
two time variable unknowns. If only it was that simple! Let’s take the
example of the 1-D linear advection equation for temperature. This
equation is represented as the following:</p>
<div class="math notranslate nohighlight" id="equation-1d-temp-eq">
<span class="eqno">(4)<a class="headerlink" href="#equation-1d-temp-eq" title="Permalink to this equation">¶</a></span>\[\frac{\partial T}{\partial t} + u \frac{\partial T}{\partial x} = 0\]</div>
<p>Using the FTCS scheme mentioned above, this equation can be approximated
as:</p>
<div class="math notranslate nohighlight">
\[\frac{T^{n+1}_{i} - T^{n}_{i}}{\Delta t} + u \frac{T^{n}_{i+1} - T^{n}_{i-1}}{2 \Delta x} = 0\]</div>
<p>It can be shown, by using Fourier Series, that:</p>
<div class="math notranslate nohighlight">
\[|\lambda_j|^2 = 1 + \alpha^2(\sin{j \Delta x}^2)\]</div>
<p>Therefore, <span class="math notranslate nohighlight">\(|\lambda_j|^2 \geq 1\)</span>, and so the scheme is said to be
absolutely unstable. What it means for a scheme to be unstable is that
if there is a slight change in the initial value, the result of the
computation will change dramatically. The stability of a scheme is
important in meteorological problems because if slight deviations from
the mathematical model caused by unavoidable errors in measurement do
not have a correspondingly slight effect on the approximate numerical
solution, the mathematical equations describing the problem will not
accurately predict the future outcome <span class="bibtex" id="id4">[ftcs_leapfrog]</span>.
For a more detailed technical explanation of the stability of this
scheme and the leapfrog scheme, please see the following article:
<a class="reference external" href="https://www.ecmwf.int/sites/default/files/elibrary/2002/16948-numerical-methods.pdf">https://www.ecmwf.int/sites/default/files/elibrary/2002/16948-numerical-methods.pdf</a>.</p>
</div>
<div class="section" id="leapfrog-scheme">
<span id="leapfrog"></span><h3>Leapfrog Scheme<a class="headerlink" href="#leapfrog-scheme" title="Permalink to this headline">¶</a></h3>
<p>This scheme is probably the most common scheme used for meteorological
problems. The “leapfrog” refers to the centred time difference which is
used in conjunction with centred space differences.</p>
<p>Taking the 1-D linear advection equation for temperature seen in
equation <a class="reference internal" href="#equation-1d-temp-eq">(4)</a>, applying this scheme results
in:</p>
<div class="math notranslate nohighlight">
\[\frac{T^{n+1}_{i} - T^{n-1}_{i}}{2 \Delta t} + u \frac{T^{n}_{i+1} - T^{n}_{i-1}}{2 \Delta x} = 0\]</div>
<p>It can be shown that this scheme is stable using a similar technique
previously mentioned. This equation can then be
rearranged for the forecast value
<span class="math notranslate nohighlight">\(T^{n+1}_{i}\)</span> <span class="bibtex" id="id5">[ftcs_leapfrog]</span>:</p>
<div class="math notranslate nohighlight">
\[T^{n+1}_{i} = T^{n-1}_{i} - u \frac{2 \Delta t}{2 \Delta x}(T^{n}_{i+1} - T^{n}_{i-1})\]</div>
<p>For the physical equation, a single initial condition <span class="math notranslate nohighlight">\(T^{0}\)</span> is
sufficient to determine the solution. One problem with the leapfrog
scheme is that two values of <span class="math notranslate nohighlight">\(T\)</span> are required to start the
computation. In addition to the physical initial condition
<span class="math notranslate nohighlight">\(T^{0}\)</span>, a computational initial condition <span class="math notranslate nohighlight">\(T^{1}\)</span> is
required. This cannot be obtained using the leapfrog scheme, so a
non-centred step is used to provide the value at <span class="math notranslate nohighlight">\(t = \Delta t\)</span>.
From which point on, the leapfrog scheme is used, however, the errors of
the first step will persist. This method, however, still retains an
error of order <span class="math notranslate nohighlight">\(\mathcal{O}(\Delta t^2)\)</span>. If you also use half of
the time step for the forward time step, followed by leapfrog time
steps; this will reduce the error introduced in the first
step <a class="bibtex reference internal" href="refs.html#leapfrog-slides-two" id="id6">[11]</a>. This will be the method
utilised within the software.</p>
</div>
<div class="section" id="nonlinear-instability">
<h3>Nonlinear Instability<a class="headerlink" href="#nonlinear-instability" title="Permalink to this headline">¶</a></h3>
<p>A major problem which occurs while dealing with nonlinear partial
differential equations is nonlinear instability. This is a problem where
there is a nonlinear interaction between atmospheric
waves <a class="bibtex reference internal" href="refs.html#nonlinear-instability" id="id7">[22]</a>.</p>
<p>An atmospheric wave is a periodic disturbance in the fields of
atmospheric variables (like geopotential height, temperature, or wind
velocity) which may either propagate (travelling wave) or not (standing
wave).</p>
<p>If one of the waves involved in this nonlinear interaction have a
wavelength less than <span class="math notranslate nohighlight">\(4 \Delta x\)</span> something called aliasing causes
a channelling of energy towards the small wavelengths. The continuous
feedback of energy leads to a catastrophic rise in the kinetic energy of
wavelengths between <span class="math notranslate nohighlight">\(2 \Delta x\)</span> and <span class="math notranslate nohighlight">\(4 \Delta x\)</span>. Within
the software, a smoothing operator, which reduces the amplitude of the
short waves while having little effect on the meteorologically important
waves, is utilised <a class="bibtex reference internal" href="refs.html#nonlinear-instability" id="id8">[22]</a>.</p>
<p>Another problem to mention before moving on is that for nonlinear
equations, the leapfrog scheme has a tendency to increase the amplitude
of the computational mode with time This can separate the space
dependence between the even and odd time steps. This problem can be
rectified by applying a Robert-Asselin Time Filter. After
<span class="math notranslate nohighlight">\(T^{n+1}\)</span> is obtained a slight time smoothing is applied to
<span class="math notranslate nohighlight">\(T^{n}\)</span>, where <span class="math notranslate nohighlight">\(\gamma\)</span> is on the order of
0.1 <a class="bibtex reference internal" href="refs.html#leapfrog-slides-two" id="id9">[11]</a>:</p>
<div class="math notranslate nohighlight">
\[T^{n} = T^{n} + \gamma(T^{n+1} - 2 T^{n} + T^{n-1})\]</div>
</div>
</div>
<div class="section" id="ensemble-prediction-system">
<h2>Ensemble Prediction System<a class="headerlink" href="#ensemble-prediction-system" title="Permalink to this headline">¶</a></h2>
<div class="section" id="introduction-1">
<span id="id10"></span><h3>Introduction<a class="headerlink" href="#introduction-1" title="Permalink to this headline">¶</a></h3>
<p>Ensemble Prediction Systems (EPS) are numerical weather prediction
systems that allow for the estimation of uncertainty in a weather
forecast, as well as, providing a better prediction for the future state
of the atmosphere. Instead of running a atmospheric dynamical simulation
once (this would be regarded as deterministic), the simulation is run
many different time with slightly different initial conditions. Due to
the high computational resources required to run these simulations, they
are often run at half the resolution of an equivalent deterministic
simulation. The ensemble prediction system has a control simulation that
doesn’t have any perturbations to the initial conditions. Each
simulation that makes up the system is called an ensemble
member <a class="bibtex reference internal" href="refs.html#intro-efs" id="id11">[17]</a>.</p>
</div>
<div class="section" id="advantages-of-eps">
<h3>Advantages of EPS<a class="headerlink" href="#advantages-of-eps" title="Permalink to this headline">¶</a></h3>
<div class="figure align-center">
<a class="reference internal image-reference" href="https://github.com/amsimp/papers/raw/master/scifest-online/project-book/Images/efs.jpg"><img alt="Visualisation of the Advantages of an Ensemble Prediction System" src="https://github.com/amsimp/papers/raw/master/scifest-online/project-book/Images/efs.jpg" style="width: 95%;" /></a>
</div>
<p>As a consequence of Chaos Theory, a tiny difference in the initial
conditions in a large system, such as the atmosphere, can result in
drastically different forecasted events, so that even with a tiny error,
it can become a large error in the forecasted future state of the
atmosphere. Even with the most accurate observations, error cannot be
avoided. Therefore, it is simply not possible to make a better forecast
or simulation. This is why an ensemble prediction system is utilised. In
an ensemble simulation, small perturbations are made to the initial
conditions, after which, the simulation is re-run. If there is a large
degree of overlap between the ensemble members, there will be a higher
degree of confidence in the ensemble forecast, with the opposite also
holding true <a class="bibtex reference internal" href="refs.html#intro-efs" id="id12">[17]</a>.</p>
</div>
<div class="section" id="global-eps">
<h3>Global EPS<a class="headerlink" href="#global-eps" title="Permalink to this headline">¶</a></h3>
<p>There are three distinct types of ensemble prediction systems: global,
regional and convective-scale. Each system address different timescales,
and different grid-sizes. A global ensemble prediction system is
designed and used for medium-range forecasting between 3 and 15 days
into the future. They use synoptic simulation models and are run at
relatively low resolutions. Although they are primarily designed for use
in the medium range, their global coverage means that they can also be
used to provide short-range EPS forecasts in regions of the globe where
no other EPS are currently available, and may be the only available
option for certain countries. Considering the software is focused on
synoptic scale simulations, this will be the system of
interest <a class="bibtex reference internal" href="refs.html#intro-efs" id="id13">[17]</a>, The default number of ensemble
members in the software is fifteen, which is typical for a global
ensemble prediction system. The grid-size, used by the software, can be
specified by the end-user in order to get a more detailed forecast (this
will increase the amount of computational resources required to run the
simulation), however, the default grid-size is of the scale of 1000 km.
There is also a hard limit of a <span class="math notranslate nohighlight">\(5^{\circ} \times 5^{\circ}\)</span> cell,
as a smaller cell size would result in inaccurate simulations due to the
fact that the dynamical equations utilised by the software do not work
on this scale. This will be discussed at greater depth in chapter
<a class="reference external" href="#4">[4]</a>.</p>
</div>
</div>
<div class="section" id="recurrent-neural-network">
<h2>Recurrent Neural Network<a class="headerlink" href="#recurrent-neural-network" title="Permalink to this headline">¶</a></h2>
<div class="section" id="introduction-2">
<span id="id14"></span><h3>Introduction<a class="headerlink" href="#introduction-2" title="Permalink to this headline">¶</a></h3>
<p>Weather forecasting has traditionally been done by physical models of
the atmosphere, which are unstable to perturbations, and thus are
inaccurate for large periods of time <a class="bibtex reference internal" href="refs.html#why-rnn" id="id15">[13]</a>. Since
machine learning techniques are more robust to perturbations, it would
be logical to combine a neural network with a physical model. Weather
forecasting is a sequential data problem, therefore, a recurrent neural
network is the most suitable option for this task.</p>
<p>A recurrent neural network is a class of artificial neural networks
where connections between nodes form a directed graph along a temporal
sequence.</p>
<p>Before, we delve into the specific example of using a recurrent neural
network to predict the future state of the atmosphere, it is necessary
to review what a recurrent neural network is. Recurrent Neural Networks
(RNNs) are neural networks that are used in situations where data is
presented in a sequence. For example, let’s say you want to predict the
future position of a fast-moving ball. Without information on the
previous position of the ball, it is only possible to make an inaccurate
guess. If you had, however, a large number of snapshots of the previous
position, you are then able to predict the future position of the ball
with some certainty. RNNs excel at modelling sequential data such as
these. This is due to sequential memory.</p>
<p>In order to intuitively understand sequential memory, the prime example
would be the alphabet. While it is easy to say the alphabet from A-Z, it
is much harder to go from Z-A. There is a logical reason why this is
difficult. As a child, you learn the alphabet in a sequence. Sequential
memory is a mechanism that makes it easier for your brain to recognise
sequence patterns.</p>
<p>In a traditional neural network, there is a input layer, hidden layer,
and a output layer. In a recurrent neural network, a loop is added that
can be added to pass information forward as seen in the diagram below
(provided by Towards Data Science) <a class="bibtex reference internal" href="refs.html#intro-rnn" id="id16">[20]</a>:</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="https://github.com/amsimp/papers/raw/master/scifest-online/project-book/Images/rnn.png"><img alt="Visualisation of a Recurrent Neural Network" src="https://github.com/amsimp/papers/raw/master/scifest-online/project-book/Images/rnn.png" style="width: 20%;" /></a>
</div>
<p>The information that is forwarded is the hidden layer, which is a
representation of previous inputs. How this works in practise is that
you initialise your network layers and the hidden the initial hidden
state. The shape and dimension of the hidden state will be dependent on
the shape and dimension of your recurrent neural network. Then you loop
through your inputs, pass the relevant parameter and hidden state into
the RNN. The RNN returns the output and a modified hidden state. Last
you pass the output to the output layer, and it returns a prediction.</p>
<p>There is, however, a major problem known as short-term memory.
Short-term memory is caused by something known as the vanishing gradient
problem, which is also prevalent in other neural network architectures.
As the RNN processes more steps, it has troubles retaining information
from previous steps. Short-Term memory and the vanishing gradient is due
to the nature of back-propagation. This can be comprehended through
understanding how a neural network is
trained <a class="bibtex reference internal" href="refs.html#intro-rnn" id="id17">[20]</a>.</p>
<p>Back-propagation is an algorithm used to train and optimise neural
networks.</p>
<p>To train a recurrent neural network, you use an application of
back-propagation called back-propagation through time. Training a neural
network has three major steps. First, the relevant data vector is
normalised between 0 and 1, the vector is feed into the RNN, and it goes
through an activation function. The activation function utilised in the
software is the rectified linear activation
function <a class="bibtex reference internal" href="refs.html#lstm-rnn" id="id18">[19]</a>.</p>
<p>The rectified linear activation function is a piece-wise linear function
that will output the input directly if is positive, otherwise, it will
output zero.</p>
<p>The function is linear for values greater than zero, meaning it has a
lot of the desirable properties of a linear activation function when
training a neural network using back-propagation. Yet, it is a nonlinear
function as negative values are always output as zero. As a result, the
rectified function is linear for half of the input domain and nonlinear
for the other half, it is referred to as a piece-wise linear
function <a class="bibtex reference internal" href="refs.html#relu" id="id19">[4]</a>. This nonlinear element is extremely
important if the system has a nonlinear component, for example in
predicting the evolution of the future state of the atmosphere.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="https://github.com/amsimp/papers/raw/master/scifest-online/project-book/Images/relu.png"><img alt="Sketch of the Rectified Linear Activation Function" src="https://github.com/amsimp/papers/raw/master/scifest-online/project-book/Images/relu.png" style="width: 95%;" /></a>
</div>
<p>Second, it outputs the results. Third, it compares the prediction to the
ground truth using a loss function.</p>
<p>A loss function outputs an error value which is an estimate of how
poorly the network is performing.</p>
<p>The lost function that will be utilised in the software will be the
function for mean squared error. The reason for choosing this particular
function is that it heavily penalises large errors, as it squares the
difference between the predicted and actual value. A large error in a
weather forecast is highly undesirable, hence, the use of this function.
The function is represented below:</p>
<div class="math notranslate nohighlight">
\[MSE = \frac{1}{n}\sum_{i=1}^n(Y_i-\hat{Y_i})^2\]</div>
<p>If a vector of <span class="math notranslate nohighlight">\(n\)</span> predictions is generated from a sample of
<span class="math notranslate nohighlight">\(n\)</span> data points on all variables, and <span class="math notranslate nohighlight">\(Y\)</span> is the vector of
observed values of the variable being predicted, with <span class="math notranslate nohighlight">\(\hat{Y_i}\)</span>
being the predicted values.</p>
<p>Mean squared error is the average squared difference between the
estimated values and the actual value.</p>
<p>Returning to the training of the RNN, it uses that error value from the
loss function. to do back propagation which calculates the gradients for
each time step in the network. The gradient is the value used to adjust
the networks internal weights, allowing the network to learn. The bigger
the gradient, the bigger the adjustments and vice versa. Here is where
the problem lies. When doing back propagation, the gradient of the
current time step is calculated with respect to the effects of the
gradients, in the time step before it. So if the adjustments to the time
step before it is small, then adjustments to the current time step will
be even smaller. The gradient values will exponentially shrink as it
propagates through each time step. That causes gradients to
exponentially shrink as it back propagates down. The earlier layers fail
to do any learning as the internal weights are barely being adjusted due
to extremely small gradients.</p>
<p>Because of vanishing gradients, the RNN doesn’t learn the long-range
dependencies across time steps. So not being able to learn on earlier
time steps causes the network to have a short-term memory. In order to
combat this, a long short-term memory is
used <a class="bibtex reference internal" href="refs.html#intro-rnn" id="id20">[20]</a>.</p>
</div>
<div class="section" id="lstm">
<h3>LSTM<a class="headerlink" href="#lstm" title="Permalink to this headline">¶</a></h3>
<p>LSTM’s were created as a solution to the short-term memory problem. They
have internal mechanisms called gates that can regulate the flow of
information. These gates can learn which data in a sequence is important
to keep or throw away. By doing that, it can pass relevant information
down the long chain of sequences to make predictions. For example, if
you were interested in buying a particular, you might read a review in
order to determine if the purchase of the product is a good decision.
When you read a review, your brain subconsciously only remembers
important keywords. You pick up words like “amazing”, “superb”, or
“awful”, you don’t remember words such as “the”, “as”, or “because”.
This is what an LSTM does, it learns to keep only the relevant
information to make predictions.</p>
<p>An LSTM has a similar control flow as a recurrent neural network. It
processes data passing on information as it propagates forward. The
differences are the operations within the LSTM’s cells. The core concept
of LSTM’s are the cell state, and it’s various gates. The cell state is
the method by which information is transferred down the sequence chain.
The cell state, in theory, can carry relevant information throughout the
processing of the sequence. So even information from the earlier time
steps can make it’s way to later time steps, reducing the effects of
short-term memory. As the cell state goes on its journey, information
get’s added or removed to the cell state via
gates <a class="bibtex reference internal" href="refs.html#lstm-rnn" id="id21">[19]</a>.</p>
<p>A gate is an electric circuit with an output which depends on the
combination of several inputs.</p>
<p>Gates contain the sigmoid activation function. The sigmoid activation
function squishes values between 0 and 1. That is helpful to update or
forget data because any number getting multiplied by 0 is 0, causing
values to disappears or be “forgotten”. Any number multiplied by 1 is
the same value therefore that value stay’s the same or is “kept”.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="https://github.com/amsimp/papers/raw/master/scifest-online/project-book/Images/sigmoid.png"><img alt="Sketch of the Sigmoid Activation Function" src="https://github.com/amsimp/papers/raw/master/scifest-online/project-book/Images/sigmoid.png" style="width: 95%;" /></a>
</div>
<p>There are three types of gates utilised within a neural network: a
forget gate, an input gate, and an output gate. A forget gate decides
what information should be thrown away or kept. Information from the
previous hidden state and information from the current input is passed
through the sigmoid function. An input gate is where the previous hidden
state and current input into a sigmoid function. The output gate decides
what the next hidden state should be. The hidden state is also used for
predictions. First, we pass the previous hidden state and the current
input into a sigmoid function. Then we pass the newly modified cell
state to the rectified linear activation function. We multiply the
rectified linear activation function output with the sigmoid output to
decide what information the hidden state should carry. The output is the
hidden state. The new cell state and the new hidden is then carried over
to the next time step <a class="bibtex reference internal" href="refs.html#lstm-rnn" id="id22">[19]</a>.</p>
</div>
<div class="section" id="implementation">
<span id="implement-rnn"></span><h3>Implementation<a class="headerlink" href="#implementation" title="Permalink to this headline">¶</a></h3>
<p>The data set for the initial conditions consists of three features:
geopotential height, air temperature, and relative humidity. For the
purposes of this specific project, the RNN will solely be trained on air
temperature and relative humidity. Unfortunately, due to the COVID-19,
there was a time constraint on the developed of the RNN, which resulted
in the inability to also train the RNN on geopotential height. This is
due to the lack of computational resources at my disposable. The data
set in question is updated every six hours by the National Oceanic and
Atmospheric Administration. This means for a single day, there will be
four observations. The goal for this project will be to, first predict
the relevant atmospheric parameter in seven days time given the last
thirty days of data and combine this RNN prediction with the physical
model prediction in an attempt to make a more accurate prediction
overall. In order to make such predictions, it is necessary to create a
window of the last 120 (<span class="math notranslate nohighlight">\(30 \times 4\)</span>) observations to train the
model <a class="bibtex reference internal" href="refs.html#time-series" id="id23">[29]</a>.</p>
<p>At the start, a seed is set in order to ensure reproducibility. As
mentioned previously, it is important to scale features before training
a neural network. Normalisation is a common way of doing this scaling by
subtracting the mean and dividing by the standard deviation of each
feature. In order for the most optimal performance, the method
“MinMaxScaler” from the library, scikit-learn, is utilised within the
software <a class="bibtex reference internal" href="refs.html#scikit-learn" id="id24">[18]</a>. An LSTM requires a
1-dimensional sequence, however, the atmosphere is a 3-dimensional
system. Hence, it is necessary to flatten the 3-dimensional vector that
represents the state of the atmosphere. This is done in order to avoid
the need of repeatably running the RNN. Batches are then created to
split the data into manageable sequences. The diagram on the following
page shows how the data is represented after flattening the data and
batching it (provided by Tensorflow) <a class="bibtex reference internal" href="refs.html#time-series" id="id25">[29]</a>.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="https://github.com/amsimp/papers/raw/master/scifest-online/project-book/Images/data_rnn.png"><img alt="Visualisation of how the data is represented after flattening and batching." src="https://github.com/amsimp/papers/raw/master/scifest-online/project-book/Images/data_rnn.png" style="width: 95%;" /></a>
</div>
<p>Following this process, the data is feed into the RNN. The LSTM model is
built using Keras in TensorFlow, which is an free and open-source
software library for machine learning. It was developed by the Google
Brain Team <a class="bibtex reference internal" href="refs.html#tensorflow" id="id26">[1]</a>. It is apparent that a
multi-step model is needed as the model needs to learn to predict a
range of future values. The source code for the LSTM model developed for
the software is shown below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Prepossessed historical data, which has been flatten and batched.</span>
<span class="n">x_data</span><span class="p">,</span> <span class="n">y_data</span> <span class="o">=</span> <span class="n">prepossessing_function</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
<span class="c1"># Prepossessed initial conditions, which has been flatten and batched.</span>
<span class="n">initial_conditions</span> <span class="o">=</span> <span class="n">prepossessing_function</span><span class="p">(</span><span class="n">input_initialconditions</span><span class="p">)</span>

<span class="c1"># The network is shown data from the last 15 days.</span>
<span class="n">past_history</span> <span class="o">=</span> <span class="mi">15</span> <span class="o">*</span> <span class="mi">4</span>

<span class="c1"># The network predicts the next 7 days worth of steps.</span>
<span class="n">future_target</span> <span class="o">=</span> <span class="mi">7</span> <span class="o">*</span> <span class="mi">4</span>

<span class="c1"># Create, and train models.</span>
<span class="c1"># Optimiser.</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">,</span> <span class="n">clipvalue</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="c1"># Create model.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span>
    <span class="n">LSTM</span><span class="p">(</span>
        <span class="mi">400</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">past_history</span><span class="p">,</span> <span class="n">features</span><span class="p">)</span>
    <span class="p">)</span>
<span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">RepeatVector</span><span class="p">(</span><span class="n">future_target</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">400</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">400</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">400</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">TimeDistributed</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">features</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mse&#39;</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;mean_absolute_error&#39;</span><span class="p">]</span>
<span class="p">)</span>

<span class="c1"># Train.</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">x_data</span><span class="p">,</span> <span class="n">y_data</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span>
<span class="p">)</span>

<span class="c1"># Predict.</span>
<span class="n">future_state</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">initial_conditions</span><span class="p">)</span>
<span class="c1"># Invert normalisation, and flattening.</span>
<span class="n">future_state</span> <span class="o">=</span> <span class="n">inverse_prepossessing</span><span class="p">(</span><span class="n">future_state</span><span class="p">)</span>
</pre></div>
</div>
<p>The model consists of four LSTM layers, which in combination are able to
produce a more accurate and reliable prediction than a single LSTM
layer. As is evident, the activation function for each LSTM is the
rectified linear activation function, which is built into Keras. The
number of epochs can be specified by the end user depending on the
computational resources they have and what they need. More epochs will
evidently lead to a more accurate neural network.</p>
<p>An epoch is one forward pass and one backward pass of all the training
examples.</p>
</div>
</div>
<div class="section" id="initial-conditions">
<span id="noaa-initial-conditions"></span><h2>Initial Conditions<a class="headerlink" href="#initial-conditions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="global-data-assimilation-system">
<h3>Global Data Assimilation System<a class="headerlink" href="#global-data-assimilation-system" title="Permalink to this headline">¶</a></h3>
<p>The initial conditions utilised by the software are from the Global Data
Assimilation System (GDAS), which is provided by the National Oceanic
and Atmospheric Adminstration in the United States. The primary reason
for utilising this data is that it is freely available to the general
public. In an ideal world, data from the European Centre for
Medium-Range Weather Forecasts would be utilised, however unfortunately,
there data is not freely available to the general public. This
fundamentally violates the software’s open source principles (these
principles are discussed in chapter <a class="reference external" href="#5">[5]</a>).</p>
<p>The GDAS is a model to place observations into a gridded model space for
the purpose of initialising weather forecasts with observed data. This
system is utilised by the National Center for Environmental Prediction
for such a purpose. GDAS adds the following types of observations to a
gridded, 3-D, model space: surface observations, balloon data, wind
profiler data, aircraft reports, buoy observations, radar observations,
and satellite observations <a class="bibtex reference internal" href="refs.html#gdas" id="id27">[8]</a>. The initial
conditions provided by the GDAS to the software have a vertical pressure
co-ordinate, or the vertical co-ordiante is pressure. This co-ordinate
system is known as isobaric co-ordinates.</p>
</div>
<div class="section" id="isobaric-co-ordinates">
<h3>Isobaric Co-Ordinates<a class="headerlink" href="#isobaric-co-ordinates" title="Permalink to this headline">¶</a></h3>
<p>In coordinate systems applied to the earth, the vertical coordinate
describes position in the vertical direction (that is, parallel to the
force of effective gravity). In meteorology, pressure can be a more
convenient vertical coordinate than altitude. One reason is that until
recently, radiosondes, which are the primary means of gathering
observations of weather variables above the earth’s surface, measure and
reported pressure, temperature, and humidity, but not altitude, as they
rise through the atmosphere <a class="bibtex reference internal" href="refs.html#isobar-i" id="id28">[5]</a>.</p>
<p>A radiosonde is an instrument carried by balloon to various levels of
the atmosphere and transmitting measurements by radio.</p>
<p>Another reason is that on scales large enough for the hydrostatic
approximation to be valid, the pressure-gradient force in
the equations of motion becomes simpler and density no longer becomes an
explicit variable in the tendency equations <a class="bibtex reference internal" href="refs.html#isobar-i" id="id29">[5]</a>.
Thus,a given geopotential gradient implies the same geostrophic wind at
any height, whereas a given horizontal pressure gradient implies
different values of the geostrophic wind depending on the
density <a class="bibtex reference internal" href="refs.html#isobar-ii" id="id30">[31]</a>.</p>
<p>From the Global Data Assimilation System, three prognostic variables are
chosen: geopotential height, air temperature, and relative humidity.</p>
<p>Geopotential Height is the height above sea level of a pressure level.
For example, if a station reports that the 500 hPa height at its
location is 5600 m, it means that the level of the atmosphere over that
station at which the atmospheric pressure is 500 hPa is 5600 meters
above sea level.</p>
<p>Geophysical sciences such as meteorology often prefer to express the
horizontal pressure gradient force as the gradient of geopotential along
a constant-pressure surface, because then it has the properties of a
conservative force. For example, the primitive equations which weather
forecast models solve use hydrostatic pressure as a vertical coordinate,
and express the slopes of those pressure surfaces in terms of
geopotential height. As such, this will be a parameter of great
interest. From the aforementioned three selected parameters, any other
parameter that is needed in the software can be calculated, including
the wind. This will be discussed in greater depth in the next chapter.</p>
</div>
</div>
</div>


          </div>
            
  <div class="footer-relations">
    
      <div class="pull-left">
        <a class="btn btn-default" href="visualisations.html" title="previous chapter (use the left arrow)">Visualisations</a>
      </div>
    
      <div class="pull-right">
        <a class="btn btn-default" href="math.html" title="next chapter (use the right arrow)">Mathematics of AMSIMP</a>
      </div>
    </div>
    <div class="clearer"></div>
  
        </div>
        <div class="clearfix"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="math.html" title="Mathematics of AMSIMP"
             >next</a> |</li>
        <li class="right" >
          <a href="visualisations.html" title="Visualisations"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">AMSIMP 0.4.2 documentation</a> &#187;</li> 
      </ul>
    </div>
<script type="text/javascript">
  $("#mobile-toggle a").click(function () {
    $("#left-column").toggle();
  });
</script>
<script type="text/javascript" src="_static/js/bootstrap.js"></script>
  <div class="footer">
    &copy; Copyright 2020, Conor Casey. Created using <a href="http://sphinx.pocoo.org/">Sphinx</a>.
  </div>
  </body>
</html>